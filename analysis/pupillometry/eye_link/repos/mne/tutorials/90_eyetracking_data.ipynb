{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "# Working with eye tracker data in MNE-Python\n",
        "\n",
        "In this tutorial we will explore simultaneously recorded eye-tracking and EEG data from\n",
        "a pupillary light reflex task. We will combine the eye-tracking and EEG data, and plot\n",
        "the ERP and pupil response to the light flashes (i.e. the pupillary light reflex).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Scott Huberty <seh33@uw.edu>\n",
        "#          Dominik Welke <dominik.welke@web.de>\n",
        "#\n",
        "#\n",
        "# License: BSD-3-Clause"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data loading\n",
        "\n",
        "As usual we start by importing the modules we need and loading some\n",
        "`example data <eyelink-dataset>`: eye-tracking data recorded from SR research's\n",
        "``'.asc'`` file format, and EEG data recorded from EGI's ``'.mff'`` file format. We'll\n",
        "pass ``create_annotations=[\"blinks\"]`` to :func:`~mne.io.read_raw_eyelink` so that\n",
        "only blinks annotations are created (by default, annotations are created for blinks,\n",
        "saccades, fixations, and experiment messages).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading C:\\Users\\arturoV\\mne_data\\eyelink-example-data\\sub-01_task-plr_eyetrack.asc\n",
            "Pixel coordinate data detected.Pass `scalings=dict(eyegaze=1e3)` when using plot method to make traces more legible.\n",
            "Pupil-size area detected.\n",
            "There are 2 recording blocks in this file. Times between  blocks will be annotated with BAD_ACQ_SKIP.\n",
            "Filtering raw data in 1 contiguous segment\n",
            "Setting up band-pass filter from 1 - 30 Hz\n",
            "\n",
            "FIR filter parameters\n",
            "---------------------\n",
            "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
            "- Windowed time-domain design (firwin) method\n",
            "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
            "- Lower passband edge: 1.00\n",
            "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
            "- Upper passband edge: 30.00 Hz\n",
            "- Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)\n",
            "- Filter length: 3301 samples (3.301 s)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.1s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
              "    <tr>\n",
              "        <th>Measurement date</th>\n",
              "        \n",
              "        <td>June 27, 2023  12:20:19 GMT</td>\n",
              "        \n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Experimenter</th>\n",
              "        \n",
              "        <td>Unknown</td>\n",
              "        \n",
              "    </tr>\n",
              "        <th>Participant</th>\n",
              "        \n",
              "        <td>Unknown</td>\n",
              "        \n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Digitized points</th>\n",
              "        \n",
              "        <td>132 points</td>\n",
              "        \n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Good channels</th>\n",
              "        <td>129 EEG, 6 Stimulus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Bad channels</th>\n",
              "        <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>EOG channels</th>\n",
              "        <td>Not available</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>ECG channels</th>\n",
              "        <td>Not available</td>\n",
              "    \n",
              "    <tr>\n",
              "        <th>Sampling frequency</th>\n",
              "        <td>1000.00 Hz</td>\n",
              "    </tr>\n",
              "    \n",
              "    \n",
              "    <tr>\n",
              "        <th>Highpass</th>\n",
              "        <td>1.00 Hz</td>\n",
              "    </tr>\n",
              "    \n",
              "    \n",
              "    <tr>\n",
              "        <th>Lowpass</th>\n",
              "        <td>30.00 Hz</td>\n",
              "    </tr>\n",
              "    \n",
              "    \n",
              "    \n",
              "    <tr>\n",
              "        <th>Filenames</th>\n",
              "        <td>signal1.bin</td>\n",
              "    </tr>\n",
              "    \n",
              "    <tr>\n",
              "        <th>Duration</th>\n",
              "        <td>00:03:11 (HH:MM:SS)</td>\n",
              "    </tr>\n",
              "</table>"
            ],
            "text/plain": [
              "<RawMff | signal1.bin, 135 x 190021 (190.0 s), ~195.9 MB, data loaded>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import mne\n",
        "from mne.datasets.eyelink import data_path\n",
        "from mne.preprocessing.eyetracking import read_eyelink_calibration\n",
        "\n",
        "et_fpath = data_path() / \"sub-01_task-plr_eyetrack.asc\"\n",
        "eeg_fpath = data_path() / \"sub-01_task-plr_eeg.mff\"\n",
        "\n",
        "raw_et = mne.io.read_raw_eyelink(et_fpath, preload=True, create_annotations=[\"blinks\"])\n",
        "raw_eeg = mne.io.read_raw_egi(eeg_fpath, preload=True, verbose=\"warning\")\n",
        "raw_eeg.filter(1, 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. seealso:: `tut-importing-eyetracking-data`\n",
        "    :class: sidebar\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The info structure of the eye-tracking data tells us we loaded a monocular recording\n",
        "with 2 eyegaze channels (x- and y-coordinate positions), 1 pupil channel, 1 stim\n",
        "channel, and 3 channels for the head distance and position (since this data was\n",
        "collected using EyeLink's Remote mode).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
              "    <tr>\n",
              "        <th>Measurement date</th>\n",
              "        \n",
              "        <td>June 29, 2023  20:07:21 GMT</td>\n",
              "        \n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Experimenter</th>\n",
              "        \n",
              "        <td>Unknown</td>\n",
              "        \n",
              "    </tr>\n",
              "        <th>Participant</th>\n",
              "        \n",
              "        <td>Unknown</td>\n",
              "        \n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Digitized points</th>\n",
              "        \n",
              "        <td>Not available</td>\n",
              "        \n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Good channels</th>\n",
              "        <td>2 Eye-tracking (Gaze position), 1 Eye-tracking (Pupil size), 1 Stimulus, 3 misc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>Bad channels</th>\n",
              "        <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>EOG channels</th>\n",
              "        <td>Not available</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <th>ECG channels</th>\n",
              "        <td>Not available</td>\n",
              "    \n",
              "    <tr>\n",
              "        <th>Sampling frequency</th>\n",
              "        <td>1000.00 Hz</td>\n",
              "    </tr>\n",
              "    \n",
              "    \n",
              "    <tr>\n",
              "        <th>Highpass</th>\n",
              "        <td>0.00 Hz</td>\n",
              "    </tr>\n",
              "    \n",
              "    \n",
              "    <tr>\n",
              "        <th>Lowpass</th>\n",
              "        <td>500.00 Hz</td>\n",
              "    </tr>\n",
              "    \n",
              "    \n",
              "</table>"
            ],
            "text/plain": [
              "<Info | 7 non-empty values\n",
              " bads: []\n",
              " ch_names: xpos_right, ypos_right, pupil_right, DIN, x_head, y_head, distance\n",
              " chs: 2 Eye-tracking (Gaze position), 1 Eye-tracking (Pupil size), 1 Stimulus, 3 misc\n",
              " custom_ref_applied: False\n",
              " highpass: 0.0 Hz\n",
              " lowpass: 500.0 Hz\n",
              " meas_date: 2023-06-29 20:07:21 UTC\n",
              " nchan: 7\n",
              " projs: []\n",
              " sfreq: 1000.0 Hz\n",
              ">"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_et.info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ocular annotations\n",
        "By default, EyeLink files will output ocular events (blinks, saccades, and\n",
        "fixations), and experiment messages. MNE will store these events\n",
        "as `mne.Annotations`. Ocular annotations contain channel information in the\n",
        "``'ch_names'`` key. This means that we can see which eye an ocular event occurred in,\n",
        "which can be useful for binocular recordings:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('xpos_right', 'ypos_right', 'pupil_right')\n"
          ]
        }
      ],
      "source": [
        "print(raw_et.annotations[0][\"ch_names\"])  # a blink in the right eye"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Checking the calibration\n",
        "\n",
        "EyeLink ``.asc`` files can also include calibration information.\n",
        "MNE-Python can load and visualize those eye-tracking calibrations, which\n",
        "is a useful first step in assessing the quality of the eye-tracking data.\n",
        ":func:`~mne.preprocessing.eyetracking.read_eyelink_calibration`\n",
        "will return a list of :class:`~mne.preprocessing.eyetracking.Calibration` instances,\n",
        "one for each calibration. We can index that list to access a specific calibration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cals = read_eyelink_calibration(et_fpath)\n",
        "print(f\"number of calibrations: {len(cals)}\")\n",
        "first_cal = cals[0]  # let's access the first (and only in this case) calibration\n",
        "print(first_cal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calibrations have dict-like attribute access; in addition to the attributes shown in\n",
        "the output above, additional attributes are ``'positions'`` (the x and y coordinates\n",
        "of each calibration point), ``'gaze'`` (the x and y coordinates of the actual gaze\n",
        "position to each calibration point), and ``'offsets'`` (the offset in visual degrees\n",
        "between the calibration position and the actual gaze position for each calibration\n",
        "point). Below is an example of how to access these data:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"offset of the first calibration point: {first_cal['offsets'][0]}\")\n",
        "print(f\"offset for each calibration point: {first_cal['offsets']}\")\n",
        "print(f\"x-coordinate for each calibration point: {first_cal['positions'].T[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot the calibration to get a better look. Below we see the location that each\n",
        "calibration point was displayed (gray dots), the positions of the actual gaze (red),\n",
        "and the offsets (in visual degrees) between the calibration position and the actual\n",
        "gaze position of each calibration point.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "first_cal.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the raw eye-tracking data\n",
        "\n",
        "Let's plot the raw eye-tracking data. We'll pass a custom `dict` into\n",
        "the scalings argument to make the eyegaze channel traces legible when plotting,\n",
        "since this file contains pixel position data (as opposed to eye angles,\n",
        "which are reported in radians).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw_et.plot(scalings=dict(eyegaze=1e3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handling blink artifacts\n",
        "\n",
        "Naturally, there are blinks in our data, which occur within ``\"BAD_blink\"``\n",
        "annotations. During blink periods, eyegaze coordinates are not reported, and pupil\n",
        "size data are ``0``. We don't want these blink artifacts biasing our analysis, so we\n",
        "have two options: Drop the blink periods from our data during epoching, or interpolate\n",
        "the missing data during the blink periods. For this tutorial, let's interpolate the\n",
        "blink samples. We'll pass ``(0.05, 0.2)`` to\n",
        ":func:`~mne.preprocessing.eyetracking.interpolate_blinks`, expanding the interpolation\n",
        "window 50 ms before and 200 ms after the blink, so that the noisy data surrounding\n",
        "the blink is also interpolated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mne.preprocessing.eyetracking.interpolate_blinks(raw_et, buffer=(0.05, 0.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. important:: By default, :func:`~mne.preprocessing.eyetracking.interpolate_blinks`,\n",
        "          will only interpolate blinks in pupil channels. Passing\n",
        "          ``interpolate_gaze=True`` will also interpolate the blink periods of the\n",
        "          eyegaze channels. Be aware, however, that eye movements can occur\n",
        "          during blinks which makes the gaze data less suitable for interpolation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract common stimulus events from the data\n",
        "\n",
        "In this experiment, a photodiode attached to the display screen was connected to both\n",
        "the EEG and eye-tracking systems. The photodiode was triggered by the the light flash\n",
        "stimuli, causing a signal to be sent to both systems simultaneously, signifying the\n",
        "onset of the flash. The photodiode signal was recorded as a digital input channel in\n",
        "the EEG and eye-tracking data. MNE loads these data as a :term:`stim channel`.\n",
        "\n",
        "We'll extract the flash event onsets from both the EEG and eye-tracking data, as they\n",
        "are necessary for aligning the data from the two recordings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "et_events = mne.find_events(raw_et, min_duration=0.01, shortest_event=1, uint_cast=True)\n",
        "eeg_events = mne.find_events(raw_eeg, stim_channel=\"DIN3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output above shows us that both the EEG and EyeLink data used event ID ``2`` for\n",
        "the flash events, so we'll create a dictionary to use later when plotting to label\n",
        "those events.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "event_dict = dict(Flash=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Align the eye-tracking data with EEG the data\n",
        "\n",
        "In this dataset, eye-tracking and EEG data were recorded simultaneously, but on\n",
        "different systems, so we'll need to align the data before we can analyze them\n",
        "together. We can do this using the :func:`~mne.preprocessing.realign_raw` function,\n",
        "which will align the data based on the timing of the shared events that are present in\n",
        "both :class:`~mne.io.Raw` objects. We'll use the shared photodiode events we extracted\n",
        "above, but first we need to convert the event onsets from samples to seconds. Once the\n",
        "data have been aligned, we'll add the EEG channels to the eye-tracking raw object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Convert event onsets from samples to seconds\n",
        "et_flash_times = et_events[:, 0] / raw_et.info[\"sfreq\"]\n",
        "eeg_flash_times = eeg_events[:, 0] / raw_eeg.info[\"sfreq\"]\n",
        "# Align the data\n",
        "mne.preprocessing.realign_raw(\n",
        "    raw_et, raw_eeg, et_flash_times, eeg_flash_times, verbose=\"error\"\n",
        ")\n",
        "# Add EEG channels to the eye-tracking raw object\n",
        "raw_et.add_channels([raw_eeg], force_update_info=True)\n",
        "\n",
        "# Define a few channel groups of interest and plot the data\n",
        "frontal = [\"E19\", \"E11\", \"E4\", \"E12\", \"E5\"]\n",
        "occipital = [\"E61\", \"E62\", \"E78\", \"E67\", \"E72\", \"E77\"]\n",
        "pupil = [\"pupil_right\"]\n",
        "picks_idx = mne.pick_channels(\n",
        "    raw_et.ch_names, frontal + occipital + pupil, ordered=True\n",
        ")\n",
        "raw_et.plot(events=et_events, event_id=event_dict, event_color=\"g\", order=picks_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Showing the pupillary light reflex\n",
        "Now let's extract epochs around our flash events. We should see a clear pupil\n",
        "constriction response to the flashes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'et_events' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mz:\\lab\\escritorio\\projects\\pupil_labs\\analysis\\pupil_labs_analysis\\analysis\\pupillometry\\eye_link\\repos\\mne\\tutorials\\90_eyetracking_data.ipynb Cell 28\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/lab/escritorio/projects/pupil_labs/analysis/pupil_labs_analysis/analysis/pupillometry/eye_link/repos/mne/tutorials/90_eyetracking_data.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m epochs \u001b[39m=\u001b[39m mne\u001b[39m.\u001b[39mEpochs(\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/lab/escritorio/projects/pupil_labs/analysis/pupil_labs_analysis/analysis/pupillometry/eye_link/repos/mne/tutorials/90_eyetracking_data.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     raw_et,\n\u001b[1;32m----> <a href='vscode-notebook-cell:/z%3A/lab/escritorio/projects/pupil_labs/analysis/pupil_labs_analysis/analysis/pupillometry/eye_link/repos/mne/tutorials/90_eyetracking_data.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     events\u001b[39m=\u001b[39met_events,\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/lab/escritorio/projects/pupil_labs/analysis/pupil_labs_analysis/analysis/pupillometry/eye_link/repos/mne/tutorials/90_eyetracking_data.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     event_id\u001b[39m=\u001b[39mevent_dict,\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/lab/escritorio/projects/pupil_labs/analysis/pupil_labs_analysis/analysis/pupillometry/eye_link/repos/mne/tutorials/90_eyetracking_data.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     tmin\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m0.3\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/lab/escritorio/projects/pupil_labs/analysis/pupil_labs_analysis/analysis/pupillometry/eye_link/repos/mne/tutorials/90_eyetracking_data.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     tmax\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/lab/escritorio/projects/pupil_labs/analysis/pupil_labs_analysis/analysis/pupillometry/eye_link/repos/mne/tutorials/90_eyetracking_data.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     preload\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/lab/escritorio/projects/pupil_labs/analysis/pupil_labs_analysis/analysis/pupillometry/eye_link/repos/mne/tutorials/90_eyetracking_data.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/lab/escritorio/projects/pupil_labs/analysis/pupil_labs_analysis/analysis/pupillometry/eye_link/repos/mne/tutorials/90_eyetracking_data.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m epochs[:\u001b[39m8\u001b[39m]\u001b[39m.\u001b[39mplot(events\u001b[39m=\u001b[39met_events, event_id\u001b[39m=\u001b[39mevent_dict, order\u001b[39m=\u001b[39mpicks_idx)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'et_events' is not defined"
          ]
        }
      ],
      "source": [
        "epochs = mne.Epochs(\n",
        "    raw_et,\n",
        "    events=et_events,\n",
        "    event_id=event_dict,\n",
        "    tmin=-0.3,\n",
        "    tmax=3,\n",
        "    preload=True,\n",
        ")\n",
        "epochs[:8].plot(events=et_events, event_id=event_dict, order=picks_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let's plot the evoked responses to the light flashes to get a sense of the\n",
        "average pupillary light response, and the associated ERP in the EEG data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs.average().plot(picks=occipital + pupil)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
